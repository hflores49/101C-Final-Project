
#Uploading data
```{r}
H1B.train <- read.csv("TrainH1BLast.csv", header = T,  na.strings=c("","NA"))
H1B.test <- read.csv("TestH1BLast No Y values.csv", header = T)
attach(H1B.train)
```

#Looking into the data
I didn't do any data analysis in R. I based all my decisions off of looking at the varaible descriptions and playing around in excel. The commented out code is from an online paper https://github.com/mihinsumaria/H1B-Case-Status-Prediction. I figured I'd keep it here incase I need it later.
```{r} 
names(H1B.train)



# library(dplyr)
# library(ggplot2)
# visa=H_1B_FY14_Q4
# case.status=visa %>% filter(!is.na(STATUS)) %>% group_by(STATUS) %>% summarise(PROPORTION=round(n()*100/nrow(visa),1))
# 
# 
# print(ggplot(data = case.status, aes(x = reorder(STATUS, PROPORTION), y = PROPORTION, fill = STATUS)) +geom_bar(stat = "identity") + geom_text(aes(label = paste(PROPORTION,"%")), vjust=0,hjust = 1) + labs(x = "Case Status", y = "Percent", title = "Status of petitioned applications") + scale_y_continuous(breaks = seq(0,100,10)) + coord_flip())
# 
# employers=visa %>% group_by(LCA_CASE_EMPLOYER_NAME) %>% summarise(count=n(),percent=round(count*100/nrow(visa),5)) %>% arrange(desc(count))
#            
# p=ggplot(data = employers[1:10,], aes(x = reorder(LCA_CASE_EMPLOYER_NAME, percent), y = percent, fill = LCA_CASE_EMPLOYER_NAME)) +geom_bar(stat = "identity") + geom_text(aes(label = paste(percent,"%")), vjust=0,hjust = 1) + labs(x = "Company", y = "Percent", title = "Applications per company") + scale_y_continuous(breaks = seq(0,100,10)) + coord_flip()
# 
# p=p+theme(legend.position = "bottom") + guides(fill=guide_legend(nrow=5,byrow=TRUE))
# print(p)
# 
# jobs=visa %>% group_by(LCA_CASE_JOB_TITLE) %>% summarise(count=n(),percent=round(count*100/nrow(visa),5)) %>% arrange(desc(count))
# 
# p=ggplot(data = jobs[1:10,], aes(x = reorder(LCA_CASE_JOB_TITLE, percent), y = percent, fill = LCA_CASE_JOB_TITLE)) +geom_bar(stat = "identity") + geom_text(aes(label = paste(percent,"%")), vjust=0,hjust = 1) + labs(x = "Job-title", y = "Percent", title = "Jobs") + scale_y_continuous(breaks = seq(0,100,10)) + coord_flip()
# 
# p=p+theme(legend.position = "bottom") + guides(fill=guide_legend(nrow=5,byrow=TRUE))
# print(p)
# 
# fulltimeprop=prop.table(table(visa$FULL_TIME_POS))
# 
# nrow(visa[which(visa$STATUS=="DENIED" | visa$STATUS=="CERTIFIED-WITHDRAWN" | visa$STATUS=="DENIED" | visa$STATUS=="WITHDRAWN" | visa$STATUS=="REJECTED" | visa$STATUS=="INVALIDATED" ),])
```

#Cleaning data
```{r}
H1B.cleaning <- H1B.train
#getting rid of visa status that aren't H-1B then removing VISA_CLASS variable
levels(H1B.cleaning$VISA_CLASS)
H1B.cleaning <- H1B.cleaning[! H1B.cleaning$VISA_CLASS %in% c("E-3 Australian", "H-1B1 Chile", "H-1B1 Singapore"),] 
droplevels(H1B.cleaning$VISA_CLASS)

#removing data for people not on a yearly salary
levels(H1B.cleaning$PW_UNIT_OF_PAY)
H1B.cleaning <- H1B.cleaning[! (H1B.cleaning$PW_UNIT_OF_PAY %in% c("Bi-Weekly", "Hour", "Month", "Week")),] 
droplevels(H1B.cleaning$PW_UNIT_OF_PAY)


#removing redundant/full of blanks/irrelevant based on variable descriptions/factors w/ large # of levels that I don't know how to minimize
H1B.cleaning <- H1B.cleaning[-c(1,5,8:11,13:22,23,25:32,35,36,38,39,41:49,51,52)]
str(H1B.cleaning)

#selecting observations that are complete cases (removing NA's)
H1B.cleaning <- na.omit(H1B.cleaning)
H1B.cleaning <- H1B.cleaning[complete.cases(H1B.cleaning),]

#converting dates so they are stored as dates
H1B.cleaning$CASE_SUBMITTED <- as.Date(H1B.cleaning$CASE_SUBMITTED, format = "%m/%d/%Y")
H1B.cleaning$DECISION_DATE <- as.Date(H1B.cleaning$DECISION_DATE, format = "%m/%d/%Y")
H1B.cleaning$EMPLOYMENT_START_DATE <- as.Date(H1B.cleaning$EMPLOYMENT_START_DATE, format = "%m/%d/%Y")
H1B.cleaning$EMPLOYMENT_END_DATE <- as.Date(H1B.cleaning$EMPLOYMENT_END_DATE, format = "%m/%d/%Y")

#creating new variables that measure the length it took to make a decision and  duration of employment
H1B.cleaning$DECISION_DURATION <- (H1B.cleaning$DECISION_DATE - H1B.cleaning$CASE_SUBMITTED)
H1B.cleaning$EMPLOYMENT_DURATION <- (H1B.cleaning$EMPLOYMENT_END_DATE - H1B.cleaning$EMPLOYMENT_START_DATE)

#removing dates now that we have durations
H1B.cleaning <- H1B.cleaning[,-(2:5)] 

#creating new variables that measure the days between all the date values
#not sure if necessary, so commented out for now.
# H1B.cleaning$DECISION_TO_START_DURATION <- (H1B.cleaning$EMPLOYMENT_START_DATE - H1B.cleaning$DECISION_DATE)
# H1B.cleaning$DECISION_TO_END_DURATION <- (H1B.cleaning$EMPLOYMENT_END_DATE - H1B.cleaning$DECISION_DATE)
# H1B.cleaning$SUBMIT_TO_START_DURATION <- (H1B.cleaning$EMPLOYMENT_START_DATE - H1B.cleaning$CASE_SUBMITTED)
# H1B.cleaning$SUBMIT_TO_END_DURATION <- (H1B.cleaning$EMPLOYMENT_END_DATE - H1B.cleaning$CASE_SUBMITTED)

#converting durations into integers (not the durations that are commented out)
H1B.cleaning$EMPLOYMENT_DURATION <- as.integer(H1B.cleaning$EMPLOYMENT_DURATION)
H1B.cleaning$DECISION_DURATION <- as.integer(H1B.cleaning$DECISION_DURATION)

#looking at # of levels in remaining variables 
str(H1B.cleaning)
lapply(H1B.cleaning, nlevels)

#removing factor levels that are no longer used by restating each factor as a factor
H1B.cleaning$EMPLOYER_STATE <- factor(H1B.cleaning$EMPLOYER_STATE) 
H1B.cleaning$SOC_NAME <- factor(H1B.cleaning$SOC_NAME)
H1B.cleaning$FULL_TIME_POSITION <- factor(H1B.cleaning$FULL_TIME_POSITION)
H1B.cleaning$PW_SOURCE <- factor(H1B.cleaning$PW_SOURCE)
H1B.cleaning$WORKSITE_STATE <- factor(H1B.cleaning$WORKSITE_STATE) 


##revaluating nlevels for SOC_NAME, EMPLOYER_STATE, and WORKSITE_STATE so there are fewer levels
#keeping SOC_NAMEs that occur more than 50 times
sort(table(H1B.cleaning$SOC_NAME), decreasing = T) #find out top levels

H1B.cleaning <- H1B.cleaning[H1B.cleaning$SOC_NAME %in% c("SOFTWARE DEVELOPERS, APPLICATIONS",	"COMPUTER SYSTEMS ANALYSTS",	"COMPUTER PROGRAMMERS",	"COMPUTER OCCUPATIONS, ALL OTHER",	"SOFTWARE DEVELOPERS, SYSTEMS SOFTWARE ",	"MANAGEMENT ANALYSTS",	"ACCOUNTANTS AND AUDITORS",	"NETWORK AND COMPUTER SYSTEMS ADMINISTRATORS",	"COMPUTER SYSTEMS ANALYST",	"MECHANICAL ENGINEERS",	"OPERATIONS RESEARCH ANALYSTS",	"FINANCIAL ANALYSTS",	"ELECTRICAL ENGINEERS",	"PHYSICIANS AND SURGEONS, ALL OTHER",	"MARKET RESEARCH ANALYSTS AND MARKETING SPECIALISTS"), ]

H1B.cleaning$SOC_NAME <- factor(H1B.cleaning$SOC_NAME) #dropping levels that aren't used anymore


#converting state levels into 7 regions 
levels(H1B.cleaning$EMPLOYER_STATE) <- list("New England" = c("CT",	"ME",	"MA",	"NH",	"RI",	"VT"), "Mid-Atlantic" = c("DE",	"MD",	"NJ",	"NY",	"PA",	"DC"), "Midwest" = c("IL",	"IN",	"IA",	"KS",	"MI",	"MN",	"MO",	"NE",	"ND",	"OH",	"SD",	"WI"), "South" = c("AL",	"AR",	"FL",	"GA",	"KY",	"LA",	"MS",	"NC",	"SC",	"TN",	"VA",	"WV"), "Southwest" = c("AZ",	"NM",	"OK",	"TX"),  "West" = c("AK",	"CO",	"CA",	"HI",	"ID",	"MT",	"NV",	"OR",	"UT",	"WA",	"WY"), "US Territory" = c("GU",	"MP",	"PR",	"VI"))

H1B.cleaning$EMPLOYER_STATE <- factor(H1B.cleaning$EMPLOYER_STATE) #dropping levels that aren't used anymore

levels(H1B.cleaning$WORKSITE_STATE) <- list("New England" = c("CT",	"ME",	"MA",	"NH",	"RI",	"VT"), "Mid-Atlantic" = c("DE",	"MD",	"NJ",	"NY",	"PA",	"DC"), "Midwest" = c("IL",	"IN",	"IA",	"KS",	"MI",	"MN",	"MO",	"NE",	"ND",	"OH",	"SD",	"WI"), "South" = c("AL",	"AR",	"FL",	"GA",	"KY",	"LA",	"MS",	"NC",	"SC",	"TN",	"VA",	"WV"), "Southwest" = c("AZ",	"NM",	"OK",	"TX"),  "West" = c("AK",	"CO",	"CA",	"HI",	"ID",	"MT",	"NV",	"OR",	"UT",	"WA",	"WY"), "US Territory" = c("GU",	"MP",	"PR",	"VI"))

H1B.cleaning$WORKSITE_STATE <- factor(H1B.cleaning$WORKSITE_STATE) #dropping levels that aren't used anymore

str(H1B.cleaning)
#removing Full_Time_Position b/c it became redundant after only considering yearly salary
H1B.cleaning <- H1B.cleaning[,-4] 
H1B.clean <- H1B.cleaning
write.csv(H1B.clean, file = "H1B.clean.csv")

range(train$WAGE_RATE_OF_PAY_FROM)

H1B.cleaning
```

#Adding new variables we created to H1B.test data
```{r}
#converting dates so they are stored as dates
H1B.test$CASE_SUBMITTED <- as.Date(H1B.test$CASE_SUBMITTED, format = "%m/%d/%Y")
H1B.test$DECISION_DATE <- as.Date(H1B.test$DECISION_DATE, format = "%m/%d/%Y")
H1B.test$EMPLOYMENT_START_DATE <- as.Date(H1B.test$EMPLOYMENT_START_DATE, format = "%m/%d/%Y")
H1B.test$EMPLOYMENT_END_DATE <- as.Date(H1B.test$EMPLOYMENT_END_DATE, format = "%m/%d/%Y")

#creating new variables that measure the length it took to make a decision and  duration of employment
H1B.test$DECISION_DURATION <- (H1B.test$DECISION_DATE - H1B.test$CASE_SUBMITTED)
H1B.test$EMPLOYMENT_DURATION <- (H1B.test$EMPLOYMENT_END_DATE - H1B.test$EMPLOYMENT_START_DATE)

#converting durations into integers (not the durations that are commented out)
H1B.test$EMPLOYMENT_DURATION <- as.integer(H1B.test$EMPLOYMENT_DURATION)
H1B.test$DECISION_DURATION <- as.integer(H1B.test$DECISION_DURATION)


```

#Seperate cleaned data into training and testing
```{r}
set.seed(91)
train.index <- sample(1:nrow(H1B.clean), nrow(H1B.clean)*.90) #sampling 90% of HB1 training data
train <- H1B.clean[train.index, ] #90% of HB1 set to train
test.train <- H1B.clean[-train.index, ] #10% of HB1 set to test

```


#Logistic Regression w/ backwards stepwise
```{r}
str(train)
glm.fit <- glm(CASE_STATUS ~ ., family = "binomial", data = train)
summary(glm.fit)
bAIC.logit.model <- step(glm.fit, direction = "backward")
bAIC.logit.pred <- predict(bAIC.logit.model, test.train, type = "terms")
bAIC.logit.pred
```

#Tree
```{r}
#Fitting Classification Trees
library(tree)


dim(train)
head(train)

#tree model
tree.train <-tree(CASE_STATUS~.,train) 
summary(tree.train)
plot(tree.train); text(tree.train,pretty=0)
tree.train
CASE_STATUS.TEST <- test.train$CASE_STATUS
#tree model prediction
tree.pred <- predict(tree.train, test.train,type="class")
table(tree.pred, CASE_STATUS.TEST)
# accuracy :
(189+91)/(189+19+9+91)

#tree model cross validation
set.seed(3)
cv.tree.train <- cv.tree(tree.train,FUN=prune.misclass)
names(cv.tree.train)
cv.tree.train

par(mfrow=c(1,2))
plot(cv.tree.train$size,cv.tree.train$dev,type="b")
plot(cv.tree.train$k,cv.tree.train$dev,type="b") #doesn't look like we need to prune b/c 5 is the best 

tree.pred.kaggle <- predict(tree.train, H1B.test, type = "class")
tree.pred.kaggle
tree.prediction1 <- data.frame("CASE_STATUS" = tree.pred.kaggle)

write.csv(tree.prediction1, file="tree.prediction1.csv") #UPLOADED TO KAGGLE W/ 87.4% ACCURACY ON 7/29
```

#PCA
```{r}
install.packages("pls")
library(pls)

head(train)

d.new <- d
d.new[, 2:6] <- log(d[2:6], 2)
d.new

#log transform wage variables
train.logt <- train

train.logt[ , c(5,7)] <- log(train.log)
```
